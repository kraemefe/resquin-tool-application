---
title: '"Assessing Response Quality in Multi-Item Scales on Political Trust and Climate Attitudes "'
author: "Arjin"
date: "2024-09-19"
output:
  html_document: default
  word_document: default
bibliography: citation.bib
biblio-style: apsr
editor_options: 
  markdown: 
    wrap: 72
---

<!-- citation.bib added to repository and defined as aspr in yaml -->

# At a glance

<!--summary of main take home messages and lessons to be learned in this tutorial-->

## Introduction

Psychological constructs, political or social attitudes as well as
behavioral patterns are often measured by using multi-item scales in
questionnaires. Multi-item scales comprise several items, questions, or
statements that assess different aspects of the same underlying
construct, i.e., gender-role attitudes or attitudes toward foreigners.
Main concerns of multi-item scales usually revolve around the validity
of the measurement instrument itself, i.e., do the several
questions/items reflect the underlying construct or in other words: Does
the scale really measure what it intends to measure? Established scales
usually underwent a series of analyses and revisions to assess and
ensure the validity and reliability of the measurement instrument.
Nevertheless, collected data from these scales can still suffer from
bias resulting from poor response behavior. Before analyzing data from
these scales and drawing conclusions regarding a substantive research
question, the quality of survey responses to these scales should be
examined to avoid bias and ensure the validity of your results. In this
tutorial, we will focus on the relationship between the concepts of
political/institutional trust and environmental attitudes which both are
measured by multi-item scales and assess the quality of given responses
to these scales.

## Data and Measurement Instruments

For this tutorial, we use data from the GESIS Panel. The GESIS Panel is
a German probability-based mixed-mode panel study which surveys
respondents every three months on a variety of topics, such as political
and social attitudes. We specifically use data from the 2nd and 3rd wave
in 2014 from a sub-sample of the GESIS Panel (n=1,222). The data
includes multi-item measurements of political trust and environmental
attitudes.

<!--side remark-->

This sub-sample of the GESIS Panel is publicly accessible as the GESIS
Panel Campus File @gesis_panel_campus_file. <!--reference added-->It
contains a random 25% sample of the GESIS panel members surveyed in 2014
and comprises only a limited selection of variables from the originial
GESIS Panel scientific use file.

Political trust is measured with a 10-item scale and a 7-point Likert
response scale:

**Trust in Institutions:** Trust in various political institutions.

How much do you personally trust the following public institutions or
groups?

**Items:**

-   bbzc078a: Trust in Bundestag

-   bbzc079a: Trust in federal government

-   bbzc080a: Trust in political parties

-   bbzc081a: Trust in judicial authorities

-   bbzc082a: Trust in police

-   bbzc083a: Trust in politicians

-   bbzc084a: Trust in media

-   bbzc085a: Trust in European Union

-   bbzc086a: Trust in United Nations

-   bbzc087a: Trust in Federal Constitutional Court

**Response Scale:** 1 = Don’t trust at all - 7 = Entirely trust

Environmental attitudes are measured with the established NEP (New
Ecological Paradigm) scale by @dunlap_et_al_2022 comprising 15 items on
different aspects of environmental or climate attitudes. The multi-item
scale uses a 5-point Likert response scale.

<!-- reference added for NEP - should be checked -->

**NEP scale:** Environmental attitudes

To what extent do you agree or disagree with the following statements?

**Items:**

-   bczd005a: NEP-scale: Approaching maximum number of humans

-   bczd006a: NEP-scale: The right to adapt environment to the needs

-   bczd007a: NEP-scale: Consequences of human intervention

-   bczd008a: NEP-scale: Human ingenuity

-   bczd009a: NEP-scale: Abuse of the environment by humans

-   bczd010a: NEP-scale: Sufficient natural resources

-   bczd011a: NEP-scale: Equal rights for plants and animals

-   bczd012a: NEP-scale: Balance of nature stable enough

-   bczd013a: NEP-scale: Humans are subjected to natural laws

-   bczd014a: NEP-scale: Environmental crisis greatly exaggerated

-   bczd015a: NEP-scale: Earth is like spaceship

-   bczd016a: NEP-scale: Humans were assigned to rule over nature

-   bczd017a: NEP-scale: Balance of nature is very sensitive

-   bczd018a: NEP-scale: Control nature

-   bczd019a: NEP-scale: Environmental disaster

**Response Scale:** 1 = Fully agree; 2 = Agree; 3 = Neither nor; 4 =
Don't agree; 5 = Fully disagree

## Assessing Response Quality and Response Quality Indicators

To ensure unbiased conclusions regarding a substantial relationship
between two construct, we advise to initially investigate the quality of
given responses to a measurement instrument. There are several
indicators which can help identify low-quality responses and assess the
response quality in multi-item scales.

In this case, we will specifically look at several indicators of
response distribution, such as:

-   n_na: the count of missing answers across multiple items per
    respondent or
-   prop_na: the proportion of missing responses across multiple items
    per respondent
-   ii_mean: the mean over multiple items per respondent
-   ii_median: the median over multiple items per respondent
-   ii_sd: the standard deviation across multiple items per respondent
-   mahal: the mahalanobis distance per respondent.
    <!-- Mouseover: "Represents the distance of a respondents' answers from the center of a multivariate normal distribution. -->

<!-- Mouseover could also be: "It calculates an average response pattern over the given set of items and produces a score for each respondent that measures how different their responses are from this average pattern. A higher score indicates that the respondent’s answers are more unusual or inconsistent compared to the group. -->

Apart from peculiarities in the response distribution of our multi-item
scales, we will further consider indicators of different response
biases, namely:

-   MRS: Indicator of mid-point response bias (tendency to select the
    neutral/middle option on a scale); indicates sum of mid-point
    responses across multiple items
    <!-- Mouseover: "Only if scale has a numeric mid-point" -->
-   ARS: Indicator of acquiescence bias (tendency to agree with
    statements irrespective of actual views); indicates sum of responses
    above the scale mid-point across multiple items
    <!-- Mouseover: "Only for scales with agree-disagree format" -->
-   ERS: Indicator of extreme response bias (tendency to select most
    extreme response options on a scale); indicates sum of responses at
    both endpoints of the multi-item scale.

<!--  DRS is mentioned as DARS in the help page but data frame returns it as DRS. I think this is a typo in the help page to be noted -->

To calculate these indicators for the assessment of response quality of
our multi-item scales, we will use the `Resquin` package in R. The
`resquin` package comprises several functions to calculate response
quality indicators for multi-item scales. The quality indicators are
calculated per respondent. Specifically, we will use the two functions
`resp_distributions` (indicators of response distribution) and
`resp_styles`(response style indicators), designed to assess response
quality based on response distribution and identifying certain response
biases.

## Getting started

To use `resquin`, we first need to install the package from the
repository of CRAN, the Comprehensive R Archive Network. For
installation, we can use the following commands:

```{r install resquin}
# Installing resquin
install.packages("resquin")
# Loading resquin into the R session
library(resquin)
```

Alongside `resquin` itself, we will use other packages for setup, data
preparation and analysis in this tutorial. To install and load these
packages from CRAN simultaneously, we will use the `pacman` package:

```{r install CRAN packages}
# Installing pacman and loading pacman into the R session
install.packages("pacman")
library(pacman)

# Install and load other CRAN packages using pacman
pacman::p_load(devtools, pak, dplyr,ggplot2,tidyr,patchwork, knitr)
```

After installation, we can import the survey data we want to analyze
regarding its response quality. For both `resp_distributions` and
`resp_styles` to calculate meaningful indicators, we need to import
survey data in a wide format, i.e., with only one row per observation
unit (respondent). For this tutorial, we import our data set directly
from github: <!--maybe let's rename the data for better usability -->

```{r import data}
# Import data from github
raw_data <- read.csv("raw-data/ZA5666_v1-0-0.csv", header=TRUE, sep=";", na.strings="NA")
```

## Inspecting data

Before delving into the analysis of response quality, let's have a first
look at the distribution of given responses to both multi-item scales:

```{r data inspection}
# Creating subset of political trust scale
start_col_trust <- which(colnames(raw_data) == "bbzc078a")
end_col_trust <- which(colnames(raw_data) == "bbzc087a")
trust <- raw_data[,start_col_trust:end_col_trust]
 
# Creating subset of NEP scale
start_col_NEP <- which(colnames(raw_data) == "bczd005a")
end_col_NEP <- which(colnames(raw_data) == "bczd019a")
NEP <- raw_data[,start_col_NEP:end_col_NEP]

# Inspect responses to political trust scale
trust_responses <- lapply(trust, function(x) table(x, useNA = "ifany"))
 
# Print the results
trust_responses

# Inspect responses to NEP 
NEP_responses <- lapply(NEP, function(x) table(x, useNA = "ifany"))
 
# Print the results
NEP_responses
```

## Data preparation

A first overview of the response distribution of both scales shows that
there are several missing values which are not defined as NA yet. For
`resquin` to calculate meaningful indicators, we have to make sure that
missings are coded to NA before we run any analyses:

```{r data preparation}
# Get unique values for responses to political trust scale
unique_values_trust <- unique(unlist(lapply(trust, unique)))

# Print the unique values sorted in an ascending order
sort(unique_values_trust)

# Recode missing values to NA for responses to political trust scale
trust <- trust %>%
  mutate(across(everything(), ~ replace(., . %in% c(-22, -33, -77, -99, -111), NA)))

# View the new recoded data frame
print(trust)

# Get unique values for responses to NEP scale
unique_values_NEP <- unique(unlist(lapply(NEP, unique)))

# Print the unique values sorted in an ascending order
sort(unique_values_NEP)

# Recode missing values to NA for responses to NEP scale
NEP <- NEP %>%
  mutate(across(everything(), ~ replace(., . %in% c(-22, -33, -77, -99, -111), NA)))

# View the new recoded data frame
print(NEP)
```

## Calculating indicators of response distribution

Now that we have prepared our data for analysis, we can proceed to the
main analysis of response quality and calculate several response quality
indicators using `resquin`. Let's first look at the response
distributions of both the institutional trust scale and the NEP scale in
greater detail by using `resp_distributions`. We can use
`resp_distributions` as follows:

```{r resp_destributions default}
# Calculate indicators of response distribution with resp_distribution

# Institutional trust
trust_distribution <- resp_distributions(trust)

# Print results for the first 10 respondents
trust_distribution[1:10,]

# Environmental attitudes 
NEP_distribution <- resp_distributions(NEP)

# Print results
NEP_distribution[1:10,]
```

<!--Info box start-->

***Package-specific feature***

`resp_distributions` returns a data frame containing the several
indicators of response distribution per respondent (displayed as
separate rows of the data frame). Inspecting the calculated indicators
for the first 10 respondents in our data frame, we see that for 1 out of
10 respondents of the institutional trust scale and for 1 out of 10
respondents of the NEP scale no parameters of central tendency (i.e.,
ii_mean, ii_median) or variability (i.e., ii_sd, mahal) were calculated.
The reason for this is that `resp_distributions` by default only
calculates response distribution indicators for respondents who do not
show any missing value in the analyzed multi-item scale. Accordingly,
for all respondents who show a value higher than 0 for the indicator
`n_na` (count of missing values), indicators of central tendency and
variability are NA.

By specifying the option `min_valid_responses`, respondents with missing
values in the multi-item scale can be included in the analysis of
response quality. `min_valid_responses` takes on a numeric value between
0 and 1 and defines the share of valid responses a respondent must have
to calculate the respective indicators of response distribution.

***Be careful!***

Generally, the more complete data we have from respondents on a certain
multi-item scale, the better! Moreover, the majority of indicators is
most meaningful when respondents show complete data across all items of
a scale compared to calculating an indicator of response distribution
for e.g., only two answered items. Nevertheless, by only including
respondents with complete data, your sample can be drastically reduced
and you might lose many observations with incomplete but "sufficient"
data (e.g., respondents who responded to 4 out of 5 questions of a
multi-item scale). To include respondents with incomplete data, you can
simply decrease the necessary number of valid responses per respondent
by specifying the `min_valid_responses` option.

We advise to specify the cut-offs regarding how many valid answers a
respondent should have depending on the number of items in your scale
and to consider higher cut-offs or excluding respondents with NAs
completely if the scale comprises only a few items (ideally fewer than
10). Nevertheless, specifying cut-offs for valid responses is more or
less arbitrary and should always be considered after looking at the
data. In any case, make sure to thoroughly document and report which
cut-off you used to exclude respondents from the analysis.

<!-- We aim to shortly explain that this option exists and what it does. But that we generally recommend to exclude respondents with NAs, especially if the number of items in the multi-item scale is below 10. -->

<!--Info box end-->

We will proceed with the inspection of response distribution indicators
only for those respondents who show no missing values for the
institutional trust and environmental attitudes scale.

<!-- I made some significant changes in the rest of the document -->

## Indicators of response distribution

To analyze the response distribution of institutional trust and
environmental attitudes across *all respondents*, we can calculate and
visualize summary statistics for each indicator in the data frame
generated by `resp_distributions`. This will help us understand the
typical behavior among respondents, as well as the variability that may
indicate unusual response patterns.

### 1. Institutional Trust Scale

Let’s begin with the institutional trust scale.

```{r summary statistics trust}
# Summarize and print results over all respondents
trust_table <- summary(trust_distribution)
kable(trust_table)

# Reshape the data for density and box plots
trust_distribution_long <- pivot_longer(trust_distribution, 
                                         cols = c("ii_mean", "ii_sd", "ii_median", "mahal"), 
                                         names_to = "Indicator", 
                                         values_to = "Value")

# Remove NAs (for those we have no calculated indicators)
trust_distribution_long <- trust_distribution_long %>%
  filter(!is.na(Value))

# Calculate mean for each Indicator
mean_values <- trust_distribution_long %>%
  group_by(Indicator) %>%
  summarize(mean_value = mean(Value, na.rm = TRUE))

# Create combined boxplot and density plot with a dashed line for each mean
ggplot(trust_distribution_long, aes(x = Value, y = after_stat(scaled))) +
  geom_density(aes(y = ..scaled..), alpha = 0.5) +
  geom_boxplot(aes(y = -0.5), width = 0.5, outlier.size = 2, color = "black", fill = "lightgray") +
  geom_vline(data = mean_values, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed") +
  
  scale_color_manual(values = c("Mean" = "black")) +
  facet_wrap(~ Indicator, scales = "free") +
  labs(title = "Combined Density and Box Plots of Indicators with Mean Values",
       x = "Value",
       y = "Scaled Density / Boxplot") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.title = element_blank())  # Remove legend title

```

The density and box plots provide a quick visualization of central
tendency and variability across all indicators. Box plots highlight key
summary statistics like the median, quartiles, and potential outliers,
while density plots complement this by showing the distribution shape
and peaks. Together, they give a fuller picture of how responses are
distributed.

The `resp_distributions` function provides two measures of **central
tendency**: `ii_mean` (average response) and `ii_median` (central
response) for each respondent.

For the 10-item scale of institutional trust (ranging from 1 to 7):

-   The mean of `ii_mean` is 3.755, and the median of `ii_mean` is
    3.800. This indicates that the distribution of `ii_mean` across all
    respondents is nearly normal, with a slight skew towards lower
    values.
-   The `ii_median` has a mean value of 3.645 and a median value of
    4.000. These values indicates an almost normal distribution for
    central values, which are neutral or leans slightly towards lower
    end of the scale.
-   Box plots for both indicators show that half of the respondents have
    scores between 3.0 and 4.5. Outliers exist at 7, representing the
    highest possible value, while lower extremes are within a normal
    range.

*In summary*, the central tendency indicators shows a balanced
distribution overall with few extreme outliers at the higher end. Most
of the respondents fall within the normal range and do not stand out as
suspicious or unusual at first glance.

`resp_distributions` also provides two measures of **variability**:
`ii_sd` (individual response variability) and `mahal` (deviation from
overall response patterns).

-   The mean `ii_sd` is 1.13, meaning, on average, respondents’ trust
    levels fluctuate by about 1 point from their own personal average.

-   The third quartile of `ii_sd` is 1.45, meaning 75% of respondents
    have moderate variability in their responses, with some showing
    higher fluctuations. Those with variability exceeding 2.5 points are
    considered extreme within the group, and these respondents may need
    further scrutiny to ensure the reliability of their data.

-   Mahalanobis distance (`mahal`) does not have a straightforward
    interpretation like `ii_sd`. However, respondents with `mahal`
    scores slightly above 5 exhibit highly dissimilar response patterns
    compared to the overall average. These outliers could flag potential
    data quality concerns and requires further examination.

*In summary*, most respondents show moderate variability indicating
consistent responding, but a few demonstrate significant deviations that
may require further investigation with additional checks to assess the
consistency and reliability of their responses.
<!-- should we mention that we will closely inspect those outlier later in the tutorial?-->
Mahalanobis distance helps identify these respondents whose patterns
deviate substantially from the group norm.

<!-- Should we still include Straightlining? -->

**Straightlining or non-differentiation**

From the standard deviation across items we can additionally infer
whether respondents show straightlining response behavior across the
multiple items of the each scale. *Straightlining or
non-differentiation* describes the response pattern of selecting the
identical answer to a series of questions or items of a scale. It can
indicate whether a respondent properly processed the respective
question(s) or used shortcuts to reduce cognitive burden which in turn
produces poor quality answers that do not represent a respondents' true
values. To get a measure for straightlining response behavior, we
generate a new indicator based on a respondents' standard deviation
across the several items:
<!-- comment on different operationalizations of straightlining, maybe even refer to literature here (as special feature) -->

```{r straightlining trust}
# Generating straightlining indicator for institutional trust scale 
trust_distribution$non_diff <- NA 
trust_distribution$non_diff[trust_distribution$ii_sd == 0] <- 1
trust_distribution$non_diff[trust_distribution$ii_sd != 0] <- 0

# Get proportion of respondents who show straightlining response behavior
proportion_nondiff <- prop.table(table(trust_distribution$non_diff))[2]
proportion_nondiff
```

The results show that about 5% of respondents show **straightlining
response behavior** in the institutional trust scale, meaning that 5% of
respondents selected an identical answer across the several items.

<!--Info box start -->

**Be careful!**

Whereas straightlining can indicate "careless" response behavior
resulting in poor quality responses, we advise to always pay attention
to the contents of the several items of a scale before drawing
conclusions. For some multi-item scales, selecting identical answers
across all the items can be plausible and reflect respondents' true
values. For example, some scales comprise items that are both positive
and negative formulated - in this case straightlining seems more
implausible and more likely represents poor quality responses. In this
case, however, all items are identically formulated and showing the
identical answer across all items can reflect genuine distrust or trust
in institutions overall.
<!--comment on additional checks to make sure, such as using response times or other response biases/styles -->

<!-- I don't know... Now reading this I'm not sure if we can confidently make this statement and maybe it would be better to include political engagement instead to have a clear case of plausible straightlining vs. a clear case of implausible straightlining (environmental attitudes scale) - What do you think? -->

<!-- maybe we could also discuss here that users should be cautious since not only perfect straightlining (literally always the same answer) but also straightlining with e.g., 8 out of 9 items with the same answer could be problematic -->

<!--Info box end -->

### 2. NEP Scale

Now we will inspect central tendency and variability indicators for NEP
scale;

```{r summary statistics NEP}
# Summarize and print results over all respondents
NEP_table <- summary(NEP_distribution)
kable(NEP_table)

# Reshape the data for density and box plots
NEP_distribution_long <- pivot_longer(NEP_distribution, 
                                         cols = c("ii_mean", "ii_sd", "ii_median", "mahal"), 
                                         names_to = "Indicator", 
                                         values_to = "Value")

# Remove non-finite values
NEP_distribution_long <- NEP_distribution_long %>%
  filter(!is.na(Value) & is.finite(Value))

# Calculate mean for each Indicator
mean_values <- NEP_distribution_long %>%
  group_by(Indicator) %>%
  summarize(mean_value = mean(Value, na.rm = TRUE))

# Create combined boxplot and density plot with a dashed line for each mean
ggplot(NEP_distribution_long, aes(x = Value, y = after_stat(scaled))) +
  geom_density(aes(y = ..scaled..), alpha = 0.5) +
  geom_boxplot(aes(y = -0.5), width = 0.5, outlier.size = 2, color = "black", fill = "lightgray") +
  geom_vline(data = mean_values, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed") +
  scale_color_manual(values = c("Mean" = "black")) +
  facet_wrap(~ Indicator, scales = "free") +
  labs(title = "Combined Density and Box Plots of Indicators with Mean Values",
       x = "Value",
       y = "Scaled Density / Boxplot") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.title = element_blank()) # Remove legend title

```

**Central tendency** measures for the 15-item NEP scale (ranging from 1
to 5):

-   The mean of `ii_mean` is 2.639, and the median of `ii_mean` is
    2.667, reflecting an almost normal distribution centered around the
    middle of the scale, ranging from 2 to 4. Respondents consistently
    selected lower-end (agree) or higher-end (disagree) responses are
    clear outliers compared to the typical pattern of the sample.
-   \`\`The `ii_median` has a mean value of 2.335 and a median value of
    2.000. This suggests a noticeable concentration of responses at 2
    and 3, with a skew towards the lower end of the scale. The box plot
    flags a median of 5 as an outlier.

*In summary*, the central tendency indicators shows a clear
concentration around the midpoints of the scale, but there are notable
outliers on both the extreme agreement and disagreement ends. This
reveals having extreme responses on average as suspicious or
extraordinary at first glance.

**Variablity** measures;

-   The mean `ii_sd` is 1.1516, which indicates that, on average,
    responses vary by about 1 point from each respondents' individual
    mean. While fluctuations up to 2 are within the normal range, no
    fluctuation (`ii_sd` = 0) is a noteworthy outlier.

-   Mahalanobis distance (`mahal`) box plot flags those respondents with
    a `mahal` score near or above 6 as having highly atypical responses,
    compared to the average responding pattern of the sample.

*In summary*, most respondents show some variability over the items,
however, few giving identical answers with no variability are outliers
in the sample. Individuals with high consistency and/or are extremely
dissimilar from average response pattern (`mahal`) requires further
investigation as they could address potential data quality concerns.

**Straightlining or non-differentiation**

Unlike the instutional trust scale, the NEP scale comprises several
reversely coded items (i.e., meaning that some of the items are
positively formulated while others are negatively worded). To better
understand the difference, we have to look at the item contents again;

[*Question:*]{.underline} To what extend do you agree or disagree with
the following statements?

[*Response Scale:*]{.underline} 1 = Fully agree; 2 = Agree; 3 = Neither
nor; 4 = Don't agree; 5 = Fully disagree

[*Items:*]{.underline}

1.  We are approaching the limit of the number of people the earth can
    support [(*Pro-environmental)*]{.underline}
2.  Humans have the right to modify the natural environment to suit
    their needs [*(Negative worded)*]{.underline}
3.  When humans interfere with nature it often produces disastrous
    consequences [(*Pro-environmental)*]{.underline}
4.  Human ingenuity will ensure that we do NOT make the earth unlivable
    [*(Negative worded)*]{.underline}
5.  Humans are severely abusing the environment
    [(*Pro-environmental)*]{.underline}
6.  There are enough resources on the planet - we just have to learn how
    to use them [*(Negative worded)*]{.underline}
7.  Plants and animals have as much right as humans to exist
    [(*Pro-environmental)*]{.underline}
8.  The balance of nature is strong enough to cope with the impacts of
    modern industrial nations [*(Negative worded)*]{.underline}
9.  Despite our special abilities humans are still subject to the laws
    of nature [(*Pro-environmental)*]{.underline}
10. The so called 'ecological crisis' facing humankind has been greatly
    exaggerated [*(Negative worded)*]{.underline}
11. The earth is like a spaceship with very limited room and resources
    [(*Pro-environmental)*]{.underline}
12. Humans were meant to rule over the rest of nature [*(Negative
    worded)*]{.underline}
13. The balance of nature is very delicate and easily upset
    [(*Pro-environmental)*]{.underline}
14. Humans will eventually learn enough about how nature works to be
    able to control it [*(Negative worded)*]{.underline}
15. If things continue on their present course, we will soon experience
    a major ecological catastrophe [(*Pro-environmental)*]{.underline}

As observed, 8 of the items are positively worded, with a response of 1
indicating a pro-environmental attitude. In contrast, the remaining 7
items are negatively worded, where a response of 1 indicates an
anti-environmental attitude. As a result, respondents showing
straightlining (i.e., giving the same response to all items) indeed
contradict attitudinal aspects of previous items, making **careless
responding** as the most likely underlying mechanism. Therefore, this
design, which includes reverse-coded items, makes straightlining for the
NEP scale more concerning than for the institutional trust scale:

```{r straightlining NEP}
# Generating straightlining indicator for institutional trust scale 
NEP_distribution$non_diff <- NA 
NEP_distribution$non_diff[NEP_distribution$ii_sd == 0] <- 1
NEP_distribution$non_diff[NEP_distribution$ii_sd != 0] <- 0
table(NEP_distribution$non_diff)

# Get proportion of respondents who show straightlining response behavior
proportion_nondiff <- prop.table(table(NEP_distribution$non_diff))[2]
proportion_nondiff
```

For the NEP scale, we can see that 0.5% of respondents show
straightlining across the several items of the scale. This finding
indeed indicates that 0.5% of respondents potentially provided poor
quality responses which do not reflect their true environmental
attitudes.

<!--Info box start-->

**Be careful!**

As the NEP scale includes items with both positive and negative
wordings, the scores alone do not directly indicate pro or
anti-environmental attitudes. To derive substantively meaningul
conclusions from response distributions (i.e., the typical and atypical
attitudes among the respondents), it's necessary to reverse-code either
the positively or negatively worded questions. This ensures that all
items reflect the same directional attitude.
<!-- should we be more specific here, and mention for example about how to infer straightlining, resp_distributions indicators etc?-->

```{r}
# Create a new data frame by copying the original NEP data
NEP_recoded <- NEP

# Reverse code the negatively worded items in the new data frame
NEP_recoded$bczd006a <- 6 - NEP_recoded$bczd006a  # Q2: Humans have the right to modify the natural environment
NEP_recoded$bczd008a <- 6 - NEP_recoded$bczd008a  # Q4: Human ingenuity
NEP_recoded$bczd010a <- 6 - NEP_recoded$bczd010a  # Q6: There are enough resources
NEP_recoded$bczd012a <- 6 - NEP_recoded$bczd012a  # Q8: The balance of nature is strong enough
NEP_recoded$bczd014a <- 6 - NEP_recoded$bczd014a  # Q10: Ecological crisis exaggerated
NEP_recoded$bczd016a <- 6 - NEP_recoded$bczd016a  # Q12: Humans were meant to rule over nature
NEP_recoded$bczd018a <- 6 - NEP_recoded$bczd018a  # Q14: Control nature
```

<!--Info box end-->

## Calculating indicators of various response styles

After investigating the response distribution of both the institutional
trust scale and the NEP scale, let's now take a closer look on
systematic response styles that can indicate poor quality responses in
multi-item scales. For this, we use the `resp_styles` function of the
`resquin` package, which calculates indicators for the following
response styles:

**Response Styles:**

-   **Mid-point response style (MRS):** Tendency to choose the mid-point
    of a response scale

-   **Acquiescence (ARS):** Tendency to agree with statements

-   **Disacquiescence (DRS):** Tendency to disagree with statements

-   **Extreme Response Style (ERS):** Tendency to select the endpoints
    of a response scale

-   **Non-extreme Response Style (NERS):** Tendency to avoid selecting
    the endpoints of a response scale

To use `resp_styles`, we first need to specify the range of the response
scale of the underlying multi-item scale or matrix question. Only with
information on the range, and therefore existence of a mid-point and the
endpoints of the response scale, `resp_styles` can calculate indicators
for the different response styles. Similar to `resp_distributions`, we
can additionally specify proportion of valid responses respondents
should have on the multi-item scale (`min_valid_responses`) to calculate
response style indicators. To enable the calculation of all response
style indicators per respondent, we only include those respondents who
show no NAs across items. We can further determine whether we want
`resp_styles` to simply return the counts of each response style across
items or if it should return the proportion of a specific response
behavior out of all the items a respondent has answered. Although the
proportion of a certain response behavior is generally more informative
than the mere count, we specify the option `normalize = FALSE` for our
analysis in this tutorial.

```{r}
# Calculating response style indicators for institutional trust
trust_respstyles <- resp_styles(trust, 1, 7, min_valid_responses = 1, normalize = FALSE)

# Print results of the first 10 respondents
trust_respstyles[1:10,]

# Calculating response style indicators for environmental attitudes
NEP_respstyles <- resp_styles(NEP, 1, 7, min_valid_responses = 1, normalize = FALSE)

# Print results of the first 10 respondents
NEP_respstyles[1:10,]
```

As with `resp_distributions` `resp_styles` returns a data frame
containing the several response style indicators per respondent
(displayed as separate rows of the data frame). Again, let's first
inspect the calculated indicators for the first 10 respondents in our
data frame: Similar to `resp_distributions` we see that for 1 out of 10
respondents of the institutional trust scale and for 1 out of 10
respondents of the NEP scale, no indicators were calculated due to our
specification of `min_valid_responses`, which only included respondents
without NA across items into the analysis.

## Indicators of response styles

To make statements about the occurrence of the specific response styles
in the institutional trust and NEP scale across *all respondents*, we
have to again calculate summary statistics for each indicator in the
data frame produced by `resp_styles`.

### 1. Institutional Trust Scale

Let’s again begin with the institutional trust scale.

```{r summary statistics trust resp_styles}
# Summarize and print results over all respondents
trust_respstyles_table <- summary(trust_respstyles)
kable(trust_respstyles_table)

# Reshape the data for density and box plots
trust__respstyles_long <- pivot_longer(trust_respstyles, 
                                         cols = c("MRS", "ARS", "DRS", "ERS","NERS"), 
                                         names_to = "Indicator", 
                                         values_to = "Value")

# Remove NAs (for those we have no calculated indicators)
trust__respstyles_long <- trust__respstyles_long %>%
  filter(!is.na(Value))

# Calculate mean for each Indicator
mean_values <- trust__respstyles_long %>%
  group_by(Indicator) %>%
  summarize(mean_value = mean(Value, na.rm = TRUE))

# Create combined boxplot and density plot with a dashed line for each mean
ggplot(trust__respstyles_long, aes(x = Value, y = after_stat(scaled))) +
  geom_density(aes(y = ..scaled..), alpha = 0.5) +
  geom_boxplot(aes(y = -0.5), width = 0.5, outlier.size = 2, color = "black", fill = "lightgray") +
  geom_vline(data = mean_values, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed") +
  
  scale_color_manual(values = c("Mean" = "black")) +
  facet_wrap(~ Indicator, scales = "free") +
  labs(title = "Combined Density and Box Plots of Indicators with Mean Values",
       x = "Value",
       y = "Scaled Density / Boxplot") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.title = element_blank())  # Remove legend title

```

Looking at the response styles indicators across all respondents of the
institutional trust scale, we see the following patterns:

xy% of respondents show a tendency to select the mid-point of the
response scale.

-  *MRS:* Out of 10 items, a typical respondents in our sample tends to select the midpoint (4) response only for 2 questions. Having 0 to 8 midpoint answers is in the normal range as suggested by the boxplot, whereas having 9 and 10 midpoint responses is considered to be outlier. 
-  *ARS:* Out of 10 items,


<!--describe the results of the 5 different response styles--> Comparing
the proportions of each systematic response style, results indicate that
our data of the institutional trust measure is mostly affected by
respondents showing XY. Overall, the high/low proportion of XY suggest
that the quality of the given responses is indeed affected by poor
response behavior in the form of xy and do not reflect respondents being
undecided or neutral when thinking about how much they trust the
different institutions.
<!-- as an example of mid-point responses, but could also be that data is not affected if proportion is low and of course other response styles can be the dominant one - please adapt -->
Looking at the remaining response behaviors, results show that the
majority of respondents did not exhibit XY <!--adapt-->.

<!--Info box -->

**Be careful** When interpreting the indicators for the different
response styles, keep in mind that disacquiescence response style (DRS)
is the direct inverse of the acquiescence response style indicators
(ARS) (the same applies for the indicators of extreme response style
(ERS) and non-extreme response style (NERS)) This means that both
indicators together cannot be meaningfully interpreted regarding the
quality of given responses. Instead, the two response styles should be
evaluated regarding their magnitude across all respondents and the
measures under investigation. In our case, this means that XY
<!-- adapt --> Be also aware that the calculation of the indicators of
acquiescence response style (ARS) and disacquiescence response style
(DARS) assumes that the response scale is positively polarized, i.e.,
higher values of the response scale reflect higher levels of agreement
with certain statements or issues.

### 2. NEP Scale

Now we will inspect response style indicators for NEP scale;

```{r summary statistics NEP resp_styles}
# Summarize and print results over all respondents
NEP_respstyles_table <- summary(NEP_respstyles)
kable(NEP_respstyles_table)

# Reshape the data for density and box plots
NEP_respstyles_long <- pivot_longer(NEP_respstyles, 
                                         cols = c("MRS", "ARS", "DRS", "ERS","NERS"), 
                                         names_to = "Indicator", 
                                         values_to = "Value")

# Remove NAs (for those we have no calculated indicators)
NEP_respstyles_long <- NEP_respstyles_long %>%
  filter(!is.na(Value))

# Calculate mean for each Indicator
mean_values <- NEP_respstyles_long %>%
  group_by(Indicator) %>%
  summarize(mean_value = mean(Value, na.rm = TRUE))

# Create combined boxplot and density plot with a dashed line for each mean
ggplot(NEP_respstyles_long, aes(x = Value, y = after_stat(scaled))) +
  geom_density(aes(y = ..scaled..), alpha = 0.5) +
  geom_boxplot(aes(y = -0.5), width = 0.5, outlier.size = 2, color = "black", fill = "lightgray") +
  geom_vline(data = mean_values, aes(xintercept = mean_value, color = "Mean"), linetype = "dashed") +
  
  scale_color_manual(values = c("Mean" = "black")) +
  facet_wrap(~ Indicator, scales = "free") +
  labs(title = "Combined Density and Box Plots of Indicators with Mean Values",
       x = "Value",
       y = "Scaled Density / Boxplot") +
  theme_minimal() +
  theme(legend.position = "right",
        legend.title = element_blank())  # Remove legend title

```

Looking at the response styles indicators across all respondents of the
NEP scale, we see the following patterns: <!--fill in -->


<!--Info box -->

**Good to know** Although `resquin` and `resp_styles` is typically
designed for multi-item scales or matrix questions that share the same
question introduction and response scale, it is also possible to
evaluate "stand-alone" survey questions (e.g., attitudes toward
governmental spending, attitudes toward social policies) from a broader
topic (e.g., political attitudes) together. If you want to do so, it is
crucial to only investigate those "stand-alone" questions together which
have the same number of response options. Also, you want to make sure
that the questions are not separated from each other in the
questionnaire but are in a consecutive order. As long as the several
questions are sequential and share the same response scale range, it is
possible to calculate meaningful indicators of certain response styles,
for example mid-point or extreme point responding.

## Conclusion and recommendations

<!-- I like the overall structure of the section but content-wise we need to discuss things differently. Let's see what results we get with the new measures and adapt then -->

Overall, the indicators suggest that while there is some variability in
responses, the data appears to be of reasonable quality. However, it's
important to consider these response styles when analyzing the
relationship between trust in institutions and other variables to ensure
that conclusions are not biased by response patterns.

**Straightlining:** High straightlining percentages are not always
indicative of low-quality data but may reflect genuine patterns in
responses. **Extreme Responding:** High extreme response rates may
represent polarized opinions rather than poor data quality. 
**Response Styles:** Indicators such as acquiescence and disacquiescence offer
insights into response tendencies but should be interpreted
contextually.

**Important Note:** Removing responses based on perceived quality
without considering the context could introduce bias into your analysis.
Always interpret these indicators carefully and consider their
implications for your research findings.

## Who are the outliers?

Outlier detecter function;

```{r outlier function}
# Define the function
flag_outliers <- function(data, variables) {
  
  # Loop through each variable
  for (var in variables) {
    # Check if the variable exists in the dataframe
    if (var %in% colnames(data)) {
      
      # Step 1: Calculate the IQR for the variable
      Q1 <- quantile(data[[var]], 0.25, na.rm = TRUE)
      Q3 <- quantile(data[[var]], 0.75, na.rm = TRUE)
      IQR_value <- Q3 - Q1
      
      # Step 2: Calculate the lower and upper bounds for outliers
      lower_bound <- Q1 - 1.5 * IQR_value
      upper_bound <- Q3 + 1.5 * IQR_value
      
      # Step 3: Create a new variable flagging outliers (1 = outlier, 0 = not an outlier)
      outlier_var <- paste0("outlier_", var)
      data[[outlier_var]] <- ifelse(data[[var]] < lower_bound | data[[var]] > upper_bound, 1, 0)
      
    } else {
      warning(paste("Variable", var, "not found in the data frame. Skipping..."))
    }
  }
  
  # Return the updated data frame
  return(data)
}
```

Now lets use it on Trust Scale response distribution indicators;

```{r outlier trust resp_distributions}
# Use the function to flag outliers
df <- flag_outliers(NEP_distribution, c("ii_mean", "ii_median", "ii_sd", "mahal"))

# View the updated data frame
print(df)

# Create a new column that flags any outliers across the specified variables
df$outlier_any <- rowSums(df[, grep("outlier_", names(df))]) > 1

# Subset the data to include only rows with outliers in at least one variable
outlier_data <- df[df$outlier_any, ]
drop_na(outlier_data)

# View the subsetted data frame with outliers
print(outlier_data)

# Subset the data to include only rows with outliers in at least one variable
outlier_data <- df[df$outlier_ii_mean == 1 | df$outlier_ii_median == 1 | df$outlier_ii_sd == 1 | df$outlier_mahal == 1, ]
# View the subsetted data frame with outliers
print(n)
drop_na(outlier_data)

```

## References
